

## 摘要

（1）基于张量代数改进卷积算法；

（2）针对 CPU 和 GPU平台特性实现高性能的张量卷积算法。

- 基于窗口序的张量卷积算法（image to window, im2win）
- 相比于传统的 image to column（im2col）变换，能够显著减少内存开销，并保证点乘窗口内存访问连续性和数据重用性。

​	CPU:首先建立了一个分析优化空间与策略选择的数学分析模型，并结合 CPU 架构特性和张量卷积计算过程设计完善该模型；然后利用 SIMD 向量指令集来加速向量乘法，并通过多线程并行来充分利用 CPU 资源；最后采用循环展开等技术来减少分支判断和函数调用开销。

​	GPU:首先根据GPU 的 CUDA 架构模型，对张量卷积算法的索引进行重构，使得该算法可以更好地映射在 GPU 的架构上；然后利用 Tiling 策略来减少全局内存访问，并利用共享内存与寄存器来缓存局部数据；最后采用微内核设计、向量化加载/存储、双缓冲与数据预取等技术来提高计算吞吐率。



---

---

## 第一章 绪论

### 1. 神经网络

神经网络是人工智能领域的一个重要分支，它是一种由多个节点相互连接形成的计算模型，其中每个节点都代表一个神经元，可以接受多个输入信号，通过对这些信号进行加权、求和并加入偏置量，输出一个经过非线性变换后的结果。

![image-20230918212746249](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230918212746249.png)

### 2. 深度学习

深度学习是使用多层结构的神经网络来学习数据特征和抽象表示，并实现强大表达和推理能力的一种方法。卷积神经网络（CNN）是一种受生物视觉系统启发而设计的深度学习模型。

卷积层是其最核心和最耗时的部分。

### 3. 卷积运算

#### 3.1直接卷积算法

直接按照卷积的定义进行计算，即将输入特征图与滤波器在空间域上进行逐元素相乘并求和。

优点：简单易懂

缺点：计算效率较低，并且难以利用现代 CPU 或 GPU的并行计算能力，在性能和并行计算方面存在很大的局限性。

#### 3.2基于 im2col 变换和通用矩阵乘法（GEMM）的卷积算法

将卷积运算转化为通用矩阵乘法（General Matrix Multiplication, GEMM）运算？

优点：充分利用现代 CPU 或 GPU 的并行计算能力，并且可以调用高度优化过的GEMM 库来执行计算。

缺点：存在一些内存开销和优化问题



#### 3.3基于快速傅里叶变换（FFT）的卷积算法

它通过对输入特征图和滤波器进行快速傅里叶变换（Fast Fourier Transform, FFT），将卷积运算从空间域转化到频域，然后在频域上进行逐元素相乘并求和，最后通过逆傅里叶变换得到输出特征图

优点：可以利用 FFT 算法降低卷积运算的计算复杂度，并且可以避免 GEMM 运算中的冗余或者稀疏性问题。

缺点：FFT 和逆 FFT 过程会增加额外的内存开销，某些情况FFT 运算可能不是最优的， 补零操作 增加计算量和内存消耗

## 4. CNN 模型加速的方法

#### 4.1 硬件层面

软件生态统一计算架构（Compute Unified Device Architecture, CUDA） GPU相比于CPU在并行运算方面更具有效率

cudnn 基于GPU平台的一种基础线性代数库

#### 4.2 软件层面

软件层面的方法主要是通过对 CNN 模型结构进行压缩、剪枝或者变种等操作，以减少模型大小或者计算量。

==张量分解==、==权重共享==、==网络剪枝==、==知识蒸馏==等。

- 张量分解的核心思想是将 CNN 模型中每一层的参数权重看成是单独的高阶张量，然后将高阶张量近似表示为若干秩一张量之和。

- 权重共享的核心思想是将 CNN 模型中具有相同功能或者相似功能的参数权重设置为相同或者接近的值，从而减少模型中需要存储和更新的参数数量。

- 网络剪枝的核心思想是在训练或者训练后对 CNN 模型中一些不重要或者冗余的参数进行删除，从而减少模型大小和计算量。

- 知识蒸馏的核心思想是利用一个已经训练好的大型 CNN 模型（称为教师模型）来指导一个较小规模的 CNN 模型（称为学生模型）的训练过程，从而使学生模型能够学习到教师模型的知识，并在保持较高精度的同时实现模型加速.

总结：从论文第一章中我了解到神经网络、深度学习、卷积神经网络它们之间的概念，随后知道在使用卷积神经网络构建模型时，耗时最久的是卷积操作，而目前最主要的卷积运算各有各的缺点，基于这个问题提出了基于张量代数改进卷积算法，并在不同的CPU和GPU平台实现这个算法。



---

---

## 第二章 基于张量代数卷积算法

### 1.传统卷积算法

直接卷积算法是最朴素的实现方式，它通过遍历输入数据的每个位置并执行卷积运算来得到输出结果。这种方法计算量大，并且存在大量重叠的计算，导致效率较低。基于 im2col 变换和 GEMM 的卷积算法将输入数据和卷积核都转化为二维矩阵形式，从而将卷积操作转化为矩阵乘法运算，进而减少计算量并提高效率。这种方法的关键在于将输入数据变换为二维矩阵，然后使用矩阵乘法进行卷积计算。这种方法虽然速度快于直接卷积算法，但是需要增加一些额外的计算开销来做数据变换。

卷积操作是卷积神经网络中最基本的操作之一，它通过在输入数据上滑动一个固定大小的窗口，并计算窗口内输入数据与卷积核的点乘来提取特征信息。



在卷积操作中，一共有三个主要的张量，输入张量（$$\Iota$$ ）、卷积核（F ）和输出张量（O ）

​    NCHW格式：  <https://zhuanlan.zhihu.com/p/465383075>

N代表数量， C代表channel，H代表高度，W代表宽度。

​	第一个元素是000，第二个元素是沿着w方向的，即001，这样下去002 003，再接着呢就是沿着H方向，即004 005 006 007...这样到019后，沿C方向，轮到了020，之后021 022 ...一直到319，然后再沿N方向。

![image-20230919104957605](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230919104957605.png)

![image-20230919155302116](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230919155302116.png)

步长：

卷积核：

### 2.基于 im2col 变换和通用矩阵乘法的卷积算法

矩阵乘法

1. im2col 变换操作需要大量的内存消耗和带宽占用。

2. im2col矩阵与卷积核矩阵尺寸差距过大导致 GEMM 操作的性能下降。

![image-20230919155945690](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230919155945690.png)

重复元素

![image-20230919160200731](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230919160200731.png)

传统卷积算法遍历输入的各个元素与卷积核相乘运算量大，

基于im2col变换采用矩阵乘法减少了计算量，变换矩阵会有额外的内存开销，im2col 矩阵含有大量的冗余元素 im2win改进了变换后的矩阵减少了内存的开销，也加快了效率。

问题：卷积操作的目的，步长和卷积核。 张量？







---

---







## 第三章 基于 im2win 变换的张量卷积在 CPU 平台上的优化实现

[**SIMD向量指令集**](https://blog.csdn.net/Cbibad/article/details/131685396)

SIMD指令集是一种[**并行计算**](http://www.ydma.com)指令集，可以在一条指令中同时对多个数据进行相同的操作。它可以将多个数据打包成一个向量，然后通过一条指令对整个向量进行操作。SIMD指令集可以在单个时钟周期内完成多个操作，从而提高计算效率。	

### 1. CPU 并行加速技术

指令级并行加速在通过同时执行多条指令来提高计算效率。

线程级并行加速主要目的是通过同时执行多个线程来提高计算机系统的效率。

数据级并行加速是一种利用多个处理单元同时执行同一个操作的并行计算技术。

### 2. 基于 im2win 变换的张量卷积实现

### 3.  基于 im2win 变换的张量卷积优化

**张量卷积算法的分析模型**

向量寄存器：每个指令同时在多个标量数据上执行操作

向量操作指令：

内存层次结构

带宽方面

**基于循环重排列和循环展开优化的张量卷积**

根据循环访问的代价不同，调整循环结构

**基于向量化和 FMA 指令优化的张量卷积**



**基于存储器分块优化的张量卷积**

将多个点乘窗口得元素分别加载到寄存器中。

**基于并行策略优化的张量卷积**

### 4. 实验与结果分析

#### 4.1 软件环境

| 环境名称 | ？                                                           |
| -------- | ------------------------------------------------------------ |
| pytorch  | PyTorch既可以看作加入了GPU支持的[numpy](https://baike.baidu.com/item/numpy/5678437?fromModule=lemma_inlink)，同时也可以看成一个拥有自动求导功能的强大的[深度神经网络](https://baike.baidu.com/item/深度神经网络/6424200?fromModule=lemma_inlink)。 |
| OpenMP   | 用于[共享内存](https://baike.baidu.com/item/共享内存/2182364?fromModule=lemma_inlink)[并行系统](https://baike.baidu.com/item/并行系统/2760705?fromModule=lemma_inlink)的[多处理器](https://baike.baidu.com/item/多处理器/53566627?fromModule=lemma_inlink)程序设计的一套指导性编译处理方案 |
| GCC      | 编译器，可支持多种语言                                       |

FLOPS：每秒浮点计算次数
$$
FLOPS = \frac{float ops}{t}
$$

$$
float ops = C_0*H_0*W_0*(2*C_i*H_f*W_f)
$$

![image-20230920154350247](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230920154350247.png)

优化后的IM2win能够很大程度的减少运行时

![image-20230920154533008](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230920154533008.png)

优化后的IM2win每秒浮点计算次数大于其他

本章：在CPU上使用并行加速技术，CPU并行加速技术分为指令级并行加速，线程级别并行加速，数据级别并行加速，了解到IM2win在CPU上的实现和优化，通过实验结果的数据对比，优化后的IM2win在运行时，每秒浮点计算次数，以及内存的消耗都要优于传统的卷积网络算法。



---

---





## 第四章 基于 im2win 变换的张量卷积在 GPU 平台上的优化实现

图形处理器（GPU）是一种高并发、高吞吐量和高效能的处理器，已经成为人工智能和深度学习领域的主要计算平台

### 1. GPU 的硬件结构

![image-20230920161246070](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230920161246070.png)

- - 硬件结构：
  - CPU 拥有众多功能模块，可以适应复杂的运算环境。
  - 而 GPU 构成相对简单，其中流处理器和显存控制器占据了绝大部分晶体管。

- - 指令级别
  - CPU 由专门为顺序串行处理而优化的少量核心组成。
  - GPU 则由数以千计的更小、更高效的核心组成，并且这些核心专门为同时处理多任务而设计。

![image-20230920161446335](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230920161446335.png)

**GPU 可以同时执行数千个线程，并在大规模数据并行问题上表现显著优于CPU。**

---





### 2. GPU 的软件编程模型

#### 2.1 CUDA架构



CUDA: 是NVIDIA 推出的一种利用 GPU 进行通用并行计算的平台和编程模型

- - 特性
  - 线程组层次：将大量线程组织成网格，每个网格由多个线程块组成，每个线程块又包含多个线程。这种划分使开发人员可以根据数据并行度和硬件资源来安排线程执行。
  - 共享内存：位于每个多处理器上、可被同一个线程块中所有线程共享的高速缓存，由开发人员显式管理。共享内存可以提高数据 访问速 度 ， 并 减 少 对 设 备 内 存 的 传 输 。
  - 屏 障 同 步 机 制 ： 通 过__syncthreads()函数实现，在该函数处，所有线程都会等待直到同一个线程块中的所有其他活动线程都到达该点.

#### 2.2 CUDA 线程层次

线程层次被设计为网格、线程块和线程三个不同的级别。

- - 层次
  - 网格（grid）：表示一个核函数的启动
  - 线程块（block）：表示在一个流多处理器中执行的一组线程
  - 线程（thread）：则表示最小的计算单元。

每个线程都执行相同的代码，并且可以访问同一块共享内存。线程块中的所有线程可以通过屏障同步机制进行协调。

**每个 SM 以每 32 个线程为一组创建、管理、调度和执行这些线程，并称之为 warp。**

#### 2.3 CUDA 内存模型

CUDA 内存模型是一种显式的内存模型，可以通过它控制 GPU 中不同层次的内存空间。

- - 内存模型
  - 全局内存：**GPU 中最大的一块存储器**，用于存储所有线程都可以访问的数据。全局内存适合处理大规模计算问题，但是访问速度较**慢**，应尽量减少对它的访问。
  - 共享内存：每个 SM 都有一定数量由线程块分配的共享内存，用于线程块内部的线程协作。共享内存可以提高程序性能，避免访问全局内存，但是需要同步机制来防止数据冲突。
  - L1 和纹理缓存：用于缓存最近访问过的数据，加快 GPU 程序执行速度。L1 缓存在每个 SM 上，纹理缓存在每个流多处理器上.
  - L2 缓存：用于处理较大规模计算问题时使用。L2 缓存在所有 SM 之间共享，比 L1 缓存在更大但是相对较慢。
  - 寄存器：GPU 中最快的一种类型的内存，每个线程都有自己独立的寄存器文件。寄存器通常用于保存频繁访问和计算结果等变量。

### 3. GPU优化技术

- 共享内存和寄存器：合理分配，避免寄存器溢出。
- 数据并行性：避免不必要的串行操作和同步操作，尽量使用向量化指令和循环展开等技术来增加指令级并行度。
- 提高占用率：合理设置线程块大小、寄存器数量、共享内存大小等参数。
- 合并访存和内存对齐：保证数据在全局内存中是连续或相邻排列，并且与缓冲区边界对齐。
- 快速指令和指令级并行：



### 4.  GPU 上张量卷积算法实现

算法

### 5.  GPU 上张量卷积算法优化设计

#### 5.1 基于 tiling 策略优化的张量卷积

Tiling 技术是一种基于数据并行性的优化技术，旨在通过将数据划分为更小的块（称为 tile）来减少内存访问冲突和提高内存带宽利用率。

Tiling 技术还可以通过动态调整 tile 的大小来进一步优化性能，以适应不同的硬件环境和计算任务。

#### 5.2 基于共享内存和寄存器优化的张量卷积

GPU 设备上的内存由四个层次结构组成：全局内存、共享内存、L1/L2 缓存（在 CUDA 中不可编程控制）和寄存器。各层次有相应的特性，

利用共享内存的特性，将数据存于共享内存中，提高性能。

#### 5.3  基于微内核设计优化的张量卷积

微内核技术是一种基于微架构设计的优化技术，通过将 GPU 的硬件架构分解为多个小型、独立的内核来实现，从而提高 GPU 的并行性和性能。

#### 5.4 基于向量化加载/存储优化的张量卷积

向量化加载/存储是一种在 GPU 上提高内存访问效率的技术，其主要目的是通过将多个数据元素一次性地加载或存储到寄存器中，从而改善数据 I/O 效率和内存带宽利用率。

#### 5.5 基于双缓冲和数据预取优化的张量卷积

双缓冲技术是指使用两个缓冲区来存储输入张量和滤波器张量以进行流水线并发计算。



---

### 6. 实验与结果分析

硬件环境： GPU 型号 GeForce RTX 3090

![image-20230920170701648](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230920170701648.png)

![image-20230920170856156](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230920170856156.png)

![image-20230920170922701](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230920170922701.png)

![image-20230920171041609](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20230920171041609.png)

运行和GFLOPS性能，和cudnn不相上下，优于cuBLAS，内存中优于其他两种



消融实验结果分析  ：在人工智能(AI)，尤其是机器学习(ML)领域，ablation 指移除AI系统的一个组件。 Ablation study 指通过研究AI系统移除某一组件之后的性能，来理解该组件对整个系统的作用。Ablation study  要求系统表现出适度退化(graceful degradation)：即使丢失或削弱某个组件系统也能保持功能继续运行。 





# 心得

​			读完这篇论文，让我学到了很多东西，这篇论文主要讲的是一种卷积算法的实现，相比起传统的卷积算法，它在内存占有率 ，效率，所用时间方面有了很大的进步。初次接触深度学习还是在毕业设计的时候，当时做了一个基于深度学习的手势识别，那个时候并不了解深度学习是什么，只知道我需要准备手势的照片，放到代码里去跑，然后得到模型最后用得到的模型去推理一张新的照片。看完这篇论文，让我知道了深度学习的发展史，深度学习是用多层结构的神经网络来学习数据特征和抽象表示，并实现强大表达和推理能力的一种方法。神经网络是一种由多个节点相互连接形成的计算模型，我们需要输入多个特征，最后经过分析，得到一个结果。而卷积神经网络是一种受生物视觉系统启发而设计的深度学习模型。在我做毕设的时候，我大概收集了三千多张照片，训练模型的时候每次都需要很久，基本上都是晚上睡觉前让电脑通宵跑模型，由于数据集过小，因此得出的模型推理效果并不是很好。但是数据集太大了有需要训练很久，因此加快卷积速度是很有必要的，这篇论文提出了一种新型的卷积算法，相比起传统的卷积算法，克服了很多传统算法的缺点。加速卷积速度的方法可以从硬件层面加速，也可以从软件层面加速，硬件方面主要使用GPU去处理图片，软件层面就是改进算法。

​		第二章中，虽然很多概念不清楚，比如张量，卷积核，步长，但是通过文章介绍的优缺点，知道算法改进的地方，例如直接卷积算法，直接通过遍历各个元素，相乘相加，这样难以利用CPU,GPU并行处理数据的能力， 之后推出了基于 im2col 变换和通用矩阵乘法的卷积算法，这个算法采用了矩阵相乘的方式，充分利用了CPU,GPU的并行处理数据的能力，但是数据转成矩阵，会增加内存的消耗，也需要时间。 从论文的图中可以看到，变换后的矩阵含有很多的重复的元素，这样会增加很多重复的运算。基于 im2win 变换的张量卷积所得到的变换后的矩阵更小，而且重复元素也少，在内存开销方面优化了很多。

​		后面讲到了该算法在CPU和GPU上的实现以及优化，具体的实现过程和实验结果统计，在CPU和GPU上根据硬件结构和软件编程模型的不同分别采用了不同的优化。在算法的实现上，感觉有点难以理解，可能有很多概念并不清楚，但是有些优化还是可以理解的，例如：基于 tiling 策略优化的张量卷积，它可以通过将数据划分为更小的块（称为 tile）来减少内存访问冲突和提高内存带宽利用率。至于如何实现这个步骤，现在还不会。在实验结果图中，根据运行时和每秒浮点计算次数的统计图可以看出该算法的实现可以大大的加快训练的速度。

​	 总的来说，这篇论文给我的帮助很大，让我了解了深度学习的方法以及改进算法的重要性。毕设使用的是yolov5，刚开始使用的CPU训练模型，后面听说使用GPU会加快训练速度，又去配环境，装cuda和cudnn, 结果速度确实快了很多，通过这篇文章，我知道了为什么GPU比CPU训练快（GPU相比于CPU在并行运算方面更具有效率），也知道了cuda(一种利用 GPU 进行通用并行计算的平台和编程模型

)和cudnn( 基于GPU平台的一种基础线性代数库 )是什么。

