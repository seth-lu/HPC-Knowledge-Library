# week-2

## 卷积运算

卷积运算：假设一个n * n维的举证A和另一个n * n维矩阵B的卷积运算就是将A中的各行中每个元素与B中对应的列中的每个元素相乘，在将所有乘的结果加起来得到一个值，这就是卷积运算。



---



## padding

输入矩阵n * n维，用f * f 的过滤器（卷积核）做卷积，会得到一个（n-f+1) * （n-f+1)的矩阵，

问题1：当多次卷积操作后，输出矩阵会变得非常小。

问题2：边缘像素点的使用次数少，在卷积时较少使用，会丢失图像边缘的许多信息。

引用padding可以解决这些问题，padding分为2种

1. Valid卷积，不填充，此时得出的矩阵大小为（n-f+1) * （n-f+1)
2. Same卷积，意味着输出和输入矩阵大小相同，此时填充p的大小为$\frac{f-1}{2}$

- 卷积核的维度一般为奇数，便于用于对称填充



---



## 卷积步长

卷积步长为在一次卷积操作后，输入张量窗口移动的大小，移动幅度的大小也会影响卷积后输出向量维度的大小，假设步长为s，输出向量维度为$\frac{n+2p-f}{s}+1$ * $\frac{n+2p-f}{s}+1$



对于$\frac{n+2p-f}{s}$不是整数的处理方法，一般选取向下取整， 意味着当输入张量的窗口移动到无效数据区时，舍弃这次卷积操作。



## 三维卷积

三维卷积操作相当于在输入张量2维的基础上，添加了通道维度，也就是说输入的张量现在是三维，卷积核也应该为三维，卷积核的通道维度应该与输入张量的通道维度相等，此时卷积操作变为将各个通道的矩阵做相应的卷积操作，最后将三个通道卷积后的结果相加。例如输入张量6 * 6 * 3 卷积核 3 * 3 * 3 得到一个4 * 4二维向量， 



存在多个卷积核(过滤器) 的时候， 将输入张量分别与多个卷积核做卷积操作，最后得到多个输出矩阵， 例如，如果存在5个卷积核，则得到的输出矩阵会有5个 ，此时输出张量为n * n * 5。



---



## 单层卷积网络

最开始输入张量为神经网络的第零层记为$a^{[0]}$ , 经过多个卷积核做卷积操作后，在加上偏差得到第一层的神经网络。

在进行卷积操作时，需要用到的有卷积核，还有padding, 步长，假设此时使用到的卷积核$f^1$ ，padding 为 $p^1$ ,步长为$s^1$

, $a^0$经过卷积后变为$a^1$ ,此时$a^1$为神经网络的第一层，则$f^1,p^1,s^1$ 为该神经网络第一层的激活值。 已知输入张量，和第一层的激活值可以得出第一层的输出张量$a^1$.这就相当于构建了一层卷积层。





## 简单卷积网络实例

输入张量$a_0$为39 * 39 * 3			$a^{[0]}_{H} = a^{[0]}_{W} = 39 , a^{[0]}_{C}=3$

$f_1 = 3$ :   	   3 * 3 的卷积核 有10个

$s^1 = 1$ : 		步长为1

$p^1 = 0$  :        使用valid卷积 

通过公式$\frac{n+2p-f}{s}+1$ 得出输出张量宽和高为37 

又因为卷积核的数量为10个 因此得出的输出张量为 37 * 37 * 10

此时的输出张量为卷积后的第一层 $a^1$ 37 * 37 * 10   $a^{[1]}_{H} = a^{[1]}_{W} = 37 , a^{[1]}_{C}=10$



此时设置第二层的激活值 ，之后在得到第二层的张量

输入张量$a_1$为37 * 37 * 10			$a^{[1]}_{H} = a^{[1]}_{W} = 37 , a^{[1]}_{C}=10$

$f_2 = 5$ :   	   5 * 5 的卷积核 有20个

$s^2 = 2$ : 		步长为2

$p^2 = 0$ :        使用valid卷积 

通过公式$\frac{n+2p-f}{s}+1$ 得出输出张量宽和高为17  

此时的输出张量为卷积后的第二层 $a^2$ 17 * 17 * 20   $a^{[2]}_{H} = a^{[2]}_{W} = 17 , a^{[2]}_{C}=20$

... .. .. ... ..

- 当步长增大的时候，输出张量的高度和宽度缩小很快，输出张量的通道数和卷积核的个数有关



- 一个简单的卷积神经网络分为三层，卷积层->池化层-> 全连接层







## 池化层

卷积网络也经常使用池化层来缩减模型的大小，提高计算速度

池化层的操作就是设置一个移动窗口的大小，并让这个窗口在输入张量中滑动，我们可以设置将这个窗口比较成一个卷积核，f = 2, s = 2,即这个窗口的大小为2 * 2， 步长为 2， 在输入张量中已步长为2移动，输出张量大小的计算方法与卷积操作一致，输出张量的值根据不同的方法来选取，如果选取为最大池化，则为这个窗口中数据的最大值。 也可以选取平均值，此时值为所有数据相加取均值。



例：
$$
输入张量： 4 * 4 \\
\begin{matrix}
& 1 &3 & 2 & 1 \\
& 2 &9 & 1 & 1 \\
& 1 &3 & 2 & 3 \\
& 5 &6 & 1 & 2 
\end{matrix}
$$
窗口大小2*2 步长为 2 ，选取最大池化 得出矩阵为
$$
\begin{matrix}
&9&2\\
&6&3
\end{matrix}
$$


## 卷积神经网络实例

输入照片大小为32 * 32 * 3

$f^1 = 5, s^1 = 1, p^1 = 0$  卷积核个数为6个

得出第一层卷积层CONV1 28 * 28 * 6 

在进行池化操作 选取$f^2 = 2 ,s^2 = 2$ 

池化后得出池化层POOl1大小变为14  * 14 * 6

通常把这两个操作后共同作为一个卷积，记为Layer1







![image-20231001172716181](C:\Users\14645\AppData\Roaming\Typora\typora-user-images\image-20231001172716181.png)

Activation shape : 张量形状

Activation Size : H * W * C

parameters: 参数



##  为什么使用卷积

通过参数共享和稀疏连接减少参数的数量，如果使用全连接层，则参数的数量非常大，如32 * 32 * 6， 第一层的输出张量为 28 * 28 * 6 

则需要1400w的参数，使用5 * 5的卷积核则只需要156个参数。



















## 读后感

​			从这些材料中，我知道了很多基本概念，例如一个简单的卷积神经网络分为通常分为3层，卷积层，池化层，和全连接层。如果直接使用全连接层时，此时需要调试的参数数量非常大，这就意味着计算机需要进行更多的运算去找到合适的参数，当时用卷积时，通过可以利用参数共享和稀疏连接的方式去减少参数的数量，较少调参的时间。在构建简单的单层卷积网络时，需要加入一些参数，如卷积核的大小，步长和padding，这些参数也叫作卷积层的激活值。这些值的大小会影响卷积后张量的大小，padding作为边缘填充，可以使你卷积操作后得到与输入张量高度和宽度相等的输出张量，这也叫做Same卷积，通过Same卷积后的输出张量的宽和高不会越来越小，也不会丢失图像边缘的许多信息。遇到边缘信息不重要时也可以使用valid卷积。 步长的大小对输出张量宽高的影响更大，他决定着在输入张量上的移动幅度的大小。卷积核的大小也会影响输出张量的宽和高，卷积核的数量会影响输出张量的通道维度的大小。因此通过这些值和输入张量的信息，便可以得到输出张量的信息。在这些数据的影响下，卷积操作后得到的新张量为一层卷积层，通过多次调节这些参数，可以得到多层卷积层。

​	（im2win 在卷积运算时，首先对输入张量进行变换，由 3 * 3 * 3变换为6 * 2 * 3 的矩阵，在窗口滑动时，窗口大小由2 *  2 变为 4 * 1， 变换后的张量高度与输出时高度一致。想比于im2col ，节省了很多内存空间，im2col，将3 * 3 * 3变换为12 *4 矩阵，卷积核变为12 * 1 矩阵。）  

​		池化层可以用来缩减模型的大小，提高计算速度。池化层的操作就是设置一个移动窗口的大小，并让这个窗口在输入张量中滑动，我们可以设置将这个窗口比较成一个卷积核，f = 2, s = 2,即这个窗口的大小为2 * 2， 步长为 2， 在输入张量中已步长为2移动，输出张量大小的计算方法与卷积操作一致，输出张量的值根据不同的方法来选取，如果选取为最大池化，则为这个窗口中数据的最大值。 也可以选取平均值，此时值为所有数据相加取均值。池化层中没有参数，只有卷积层中才有参数，通常把一次卷积和一次池化共同作为一次卷积。通过这些材料渐渐补全了自己对于卷积的一些基本概念，也知道为什么需要使用卷积去去构建神经网络。
