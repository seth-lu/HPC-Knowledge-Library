读了这段材料我主要是明白了下面这些基本的概念。

# 卷积神经网络

争对问题：一张图片的数据量太大，如果使用全连接层参数会很多，难以获得足够数据来防止过拟合。要处理这样的数据,内存需求难以接受。

## 卷积如何运算（用了一个边缘检测示例）

卷积运算：输出张量的每个元素是输入张量对应部分和卷积核每行每列每通道对应的元素相乘然后将结果累加获得的。

例子用的灰度图（只有一个通道），用了一个3×3的核（过滤器）来检测图片垂直边缘。通过卷积操作可以把灰度图黑的部分和白的部分变灰，而让黑白的边界变白，以此得到图片的垂直边缘。通过使用不同的过滤器可以实现不同的效果。

## Padding

卷积操作有两个缺点

- 每次卷积操作后，图片会变小。n×n的图片用f×f的过滤器卷积后得到图像是（n-f+1）×（n-f+1）。卷积操作次数多了可能图像就比希望的小了。
- 图片边缘部分的特征会丢失，因为边缘的像素点参与的运算次数最少。

为了解决这些问题，可以在卷积操作前填充图片，往图片边缘填充像素可以控制卷积后获得的图片的大小。

通常两个选择，Valid卷积和Same卷积。Valid卷积意味着不填充，输入n×n的图片用f×f的过滤器卷积得到（n-f+1）×（n-f+1）的图像。

Same卷积意味着输出大小和输入一样。需要填充p个像素$p=(f-1)/2$。如果f是偶数的话，只能使用不对称的填充，左边/右边少填充一点。所以一般f是奇数，四周填充相同数量能保证输出图片大小可以等于输入图片大小。

## 卷积步长

前面都是默认步长为1，把卷积运算看成卷积核在输入张量上滑动，每次可以得到一个值（输出张量的一个元素），步长为一就是每次移动一个元素那么长的位置。使用不同的步长得到的输出张量不同，步长为S时输出张量的大小为$[1×\frac{n+2p-f}{s}+1]×[1×\frac{n+2p-f}{s}+1]$。如果商不是整数就向下取整。

还有一个叫“互相关”的操作，比深度学习定义的卷积多了个翻转操作，不过是运用在其他领域。深度学习中不重要

## 三维卷积

上面都是二维的。灰度图只要一个通道，但常规的图有红绿蓝三个通道，所以需要三维卷积操作。三维卷积比起二维卷积，输入张量的和卷积核对应通道的都做成绩累加，也就是一个三维卷积和三维图片卷积运算后得到二维图像。多个卷积核和输入张量卷积运算后获得对应通道数的输出张量，比如文中图片的例子是两个3×3×3的卷积核和6×6×3的输入张量，得到的是4×4×2的输出张量。

## 单层卷积网络

对输入张量用这层对应的过滤器处理，每个元素增加相同的偏差，然后应用非线性函数。不同过滤器可以增加不同偏差。然后将得到的矩阵堆叠起来就是一层卷积神经网络。

激活值：这一层的输出

偏差参数：实数，每个过滤器一个偏差参数

## 简单卷积神经网络

典型的卷积神经网络有三层，卷积层（Conv），池化层（POOL），全连接层（FC）。

## 池化层

用于缩小模型大小，提高计算速度，提高所提取特征的鲁棒性。

比如最大池化层，把输出等分成输出的大小的区域，然后取每个区域最大的数作为输出的值。池化层就是取最能代表每个区域的值。

最大池化层用的最多，因为实验中效果好。数字大意味着可能探测到了某些特定的特征，如果在过滤器中提取到某个特征，那么保留其最大值。如果没有提取到这个特征，可能在这块区域中不存在这个特征，那么其中的最大值也还是很小。

另外还有一种类型的池化，平均池化，顾名思义取平均值。

## 为什么使用卷积

只用全连接层相比，卷积层的两个主要优势在于参数共享和稀疏连接。

图片纬度很大，需要训练的参数会特别多。使用卷积层参数就少很多。因为卷积层参数共享，每个区域都使用同样的过滤器，共享过滤器的参数。然后是稀疏连接，每个输出只和输入的过滤器大小的像素有关，其他像素对其完全没有影响。神经网络可以通过这两种机制减少参数，以便我们用更小的训练集来训练它，从而预防过度拟合。

## 和im2win的关系

池化层是静态的，卷积层花费最多。im2win用于卷积操作，高效的进行卷积计算有助于提高卷积神经网络的效率。

