# 卢帅师兄的毕业论文--基于张量代数的卷积神经网络性能优化

## 名词：

基于窗口序的张量卷积算法（image to window, im2win）

image to column（im2col）

Tensor“张量”，我们通常需要处理的数据有零维的（单纯的一个数字）、一维的（数组）、二维的（矩阵）、三维的（空间矩阵）、还有很多维的。Pytorch为了把这些各种维统一起来，所以起名叫张量

单指令多数据流（SIMD）向量指令集

统一计算设备架构（Compute Unified Device Architecture，CUDA）

## 关键词：

卷积算法，张量代数，卷积神经网络，并行计算，高性能计算

## 摘要

### 针对问题：

卷积操作是卷积神经网络性能优化的重点，传统的卷积算法在处理高维数据、不同尺寸数据、不同形式数据时，往往会导致冗余计算、内存浪费和灵活性不足等问题

### 解决方式：

（1）基于张量代数改进卷积算法；

（2）针对 CPU 和 GPU平台特性实现高性能的张量卷积算法。

### 该论文工作：

（1）提出了一种新的基于张量代数的卷积算法，即基于窗口序的张量卷积算法（image to window, im2win），由内存高的im2win 变换和后续的张量乘法两部分组成。im2win 变换将输入图像按照点乘窗口运算访问顺序重新排列数据。通过im2win 变换得到的 im2win 张量是一个四维张量（传统的 im2col 变换得到的二维矩阵），并使得卷积实现更加灵活适应不同硬件平台。

（2）在 CPU 平台上实现了高性能的基于 im2win 变换的张量卷积算法。根据 CPU 平台特性采用了一系列优化技术。首先，建立了一个分析优化空间与策略选择的数学分析模型。利用SIMD 向量指令集来加速向量乘法。通过多线程并行来充分利用 CPU 资源。采用循环展开等技术来减少分支判断和函数调用开销。

该论文提出的基于 im2win 变换的张量卷积算法相比于基于 im2col 变换的卷积算法可以实现 2.2 倍到 5.8 倍的性能提升，相比于直接卷积算法可以实现 3.8 倍到 7.1 倍的性能提升；在内存占用方面，本文提出的张量卷积算法相比于基于 im2col 变换的卷积算法平均降低了41.6%的内存占用

（3）在 GPU 平台上实现基于 im2win 变换的张量卷积算法。根据GPU 的 CUDA 架构模型，对张量卷积算法的索引进行重构，使得该算法可以更好地映射在 GPU 的架构上。利用 Tiling 策略来减少全局内存访问，并利用共享内存与寄存器来缓存局部数据。采用微内核设计、向量化加载/存储、双缓冲与数据预取等技术来提高计算吞吐率。

该论文提出的张量卷积算法相比于基于 im2col 变换的卷积算法平均可以实现约为3.5 倍的性能提升，相比于 cuDNN 库的卷积算法最高可以实现约为 1.8 倍的性能提升，平均性能提升为 1.1 倍；在内存占用方面，本文提出的张量卷积算法相比于基于 im2col 变换的卷积算法平均降低了 23.1% 的内存占用，相比于cuDNN 库的卷积算法平均降低了 23.1% 的内存占用。

## 绪论

### 研究背景和意义：

卷积运算是深度学习中的核心操作，它有三种主要的实现方式：直接卷积算法、基于 im2col 变换和通用矩阵乘法（GEMM）的卷积算法和基于快速傅里叶变换（FFT）的卷积算法。

传统算法存在的问题：在处理高维数据时会产生大量冗余计算；在处理不同尺寸或稀疏数据时会造成内存浪费；在处理不同形式或结构化数据时会缺乏灵活性等。

近年来，基于张量代数的方法已成为探索 CNN 性能优化的研究热点。

### 该论文课题的研究现状：

国内外学者对卷积神经网络性能优化进行了大量的研究，主要从卷积算法、网络结构、硬件平台等方面进行改进和创新。

目前针对CNN 模型加速的方法主要可以分为两类：硬件层面的方法和软件层面的方法。

### 该论文的组织架构：

第 1 章：绪论。这一章首先介绍卷积神经网络的应用背景，以及卷积神经网络模型训练和推理中存在的性能问题，从而引出卷积神经网络性能优化的必要性和研究价值。然后，在研究现状部分主要介绍卷积神经网络性能优化和卷积操作性能优化上的国内外研究现状。最后，介绍本文的主要工作，以及本文各章节的内容安排。

第 2 章：一种新的基于张量代数的卷积算法。这一章首先介绍卷积操作的定义以及传统的卷积实现算法，并分析现有的传统卷积算法存在的不足和缺点。然后，为了解决传统卷积算法的内存占用过大和内存访问不连续的缺点，提出一种基于窗口序排列的张量数据转置算法来降低内存占用并保持内存访问连续。最后，将本文提出的基于窗口序排列的张量数据转置算法与传统的数据转置算法进行对比分析。

第 3 章：基于窗口序的张量卷积在 CPU 平台上的优化实现。这一章首先介绍 CPU 的硬件体系结构、SIMD 向量指令集以及 CPU 并行技术。然后，根据CPU 的硬件体系结构特点和并行模型，在 CPU 上设计实现本文提出的基于窗口序的张量卷积算法，并采用了 SIMD 向量指令集、多线程并行、循环展开等优化技术来提高运行效率。最后，在 CPU 设备上使用一个完备的测试基准对实现的基于窗口序的张量卷积算法与现有方法之间进行性能对比，包括运行时间、每秒浮点运算次数和内存占用三个指标，并分析本文提出的算法的通用性和有效性。

第 4 章：基于窗口序的张量卷积在 GPU 平台上的优化实现。这一章首先介绍了 GPU 的硬件体系结构、软件编程模型以及计算统一设备架构 CUDA。然后，根据 GPU 的硬件体系结构特点和计算统一设备架构 CUDA，在 GPU 上设计实现本文提出的基于窗口序的张量卷积算法，并采用了 Tiling 策略、共享内存与寄存器、微内核设计、向量化加载/存储、双缓冲与数据预取等优化技术来进 提高运行效率。最后，在一个完备的测试基准对实现的基于窗口序的张量卷积算法与现有方法进行性能测试，指标包括运行时间、每秒浮点运算次数和内存占用，并且对本文实现的基于窗口序的张量卷积算法进行消融实验，以分析本文提出的算法的通用性和有效性。

第 5 章：总结与展望。这一章对本文的研究内容进行概括总结。

## 一种新的基于张量代数的卷积算法

### 传统的卷积算法：

传统的卷积算法包括直接卷积算法以及基于im2col 变换和通用矩阵乘法（GEMM）的卷积算法。

直接卷积算法是最朴素的实现方式，它通过遍历输入数据的每个位置并执行卷积运算来得到输出结果。这种方法计算量大，并且存在大量重叠的计算，导致效率较低。

基于 im2col 变换和 GEMM 的卷积算法将输入数据和卷积核都转化为二维矩阵形式，从而将卷积操作转化为矩阵乘法运算，进而减少计算量并提高效率。这种方法的关键在于将输入数据变换为二维矩阵，然后使用矩阵乘法进行卷积计算。这种方法虽然速度快于直接卷积算法，但是需要增加一些额外的计算开销来做数据变换。

#### 卷积操作定义：

卷积操作是卷积神经网络中最基本的操作之一，它通过在输入数据上滑动一个固定大小的窗口，并计算窗口内输入数据与卷积核的点乘来提取特征信息。

在卷积操作中，一共有三个主要的张量，输入张量（i）、卷积核（f）和输出张量（o）。

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230918213254.png)

#### 直接卷积算法：

直接使用原始的高维张量进行计算。

#### 基于im2col变换和通用矩阵乘法的卷积运算：

基于 image to column（im2col）变换和通用矩阵乘法（GEMM）卷积（以下简称为基于 im2col 变换的卷积）是目前最常用的卷积算法。

基于 im2col 变换的卷积本质上是将原本的卷积操作转换成一个 GEMM 操作。

由于该卷积算法的实现较为简单且性能良好，它经常被用作与其他卷积算法性能比较的基准。

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230919104601.png)

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230919104618.png)

### 一种新的基于张量代数的卷积算法

该算法由一种新的 im2win 变换和后续的张量乘法两部分组成。

#### 内存高效的im2win变换：

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230919105600.png)

im2win 变换除了能够降低内存消耗并保证点乘窗口内存访问连续外，还可以在后续卷积操作计算过程中提高数据重用。在连续的两个点乘窗口操作中，第一个操作中加载的大部分元素会在第二个操作中被重复使用，这可以极大地提高数据重用和高速缓存的命中率。

im2win变换与传统的 im2col 变换得到的 im2col 矩阵对比能够有效降低内存空间的消耗，并且相比于原始输入张量能够保证每个点乘窗口的元素内存访问连续。

#### 基于 im2win 变换的张量卷积：

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921093326.png)

但 im2win 张量中数据排列是按照连续的点乘窗口排列，因此在基于 im2win 变换的张量卷积实现时可以一次运算多个窗口而不是单个窗口来提升卷积操作的运算效率。

#### im2win 变换算法的对比分析：

## 基于 im2win 变换的张量卷积在 CPU 平台上的优化实现

建立了一个优化分析模型，并采用了 SIMD 向量指令集、多线程并行、循环展开等技术来加速矩阵乘法和充分利用 CPU 资源。

### CPU 平台的体系结构

SIMD 向量指令集作为一种并行计算技术，可以同时对多个数据进行相同的操作，从而提高卷积算法的计算效率。利用多核 CPU 的并行计算能力，可以使卷积算法实现更快的计算速度和更高的能效比。

####  CPU 的硬件结构：

在像卷积操作这样的计算密集型操作中，CPU 的浮点计算能力通常并不是性能瓶颈。相反，数据从主存搬运到寄存器中的速度往往无法跟上浮点计算的速度，因此对于卷积算法的性能优化而言，合理分配存储器并优化数据加载方式更为重要。

在卷积算法中，由于局部性原理的存在，使用高速缓存可以大大降低访问延迟，并提高算法的性能。同时，将数据尽可能地保存在寄存器中也是提高算法性能的一种有效手段。

然而，高速缓存容量和成本的限制也会影响算法的设计，因为当数据量超出高速缓存容量时，可能需要频繁地从主存中获取数据，进而带来额外的访问延迟。因此，在后续卷积算法设计和优化过程中，本文根据内存层次结构的特点和限制来优化算法访问内存的方式，以达到最优的算法性能。

#### SIMD 向量指令集：

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921100619.png)

单指令多数据流（SIMD）向量指令集是一种计算机处理器指令集，其设计思想是将多个数据元素打包成一个向量，以单条指令的方式同时对这些向量进行操作，从而实现高效的并行计算。

相比于传统的标量指令，SIMD 指令集能够更好地利用处理器中的硬件资源和执行单元，提高计算效率和吞吐量，特别是在并行执行卷积操作这种部分数据类似或相同的运算时。

流式 SIMD 扩展（SSE）和高级向量扩展（AVX）是 Intel x86 架构上广泛使用的两种 SIMD 指令集。

AVX 指令集是 SSE 指令集的扩展，在保留了 SSE 所有功能和兼容性的同时，支持更长的向量长度（256 位），新增了更多丰富而强大的功能和优化。AVX 最大的亮点是 Fused Multiply-Add（FMA）指令，可在一条指令内完成乘加法操作，并且减少了中间结果舍入误差，从而提高浮点运算效率和精度。

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921142129.png)

在程序中使用这些SIMD 指示进行编程时有三种方法：（1）编译器自动向量化技术；（2）Intrinsics函数；（3）Assembly 汇编语句。

#### CPU 并行加速技术：

这些技术旨在通过同时执行多个计算任务以最大限度地利用 CPU 资源，从而提高计算效率，满足超大规模复杂计算任务的需求。

##### 指令级并行加速

指令级并行加速（Instruction-Level Parallelism，ILP）旨在通过同时执行多条指令来提高计算效率。

由于张量卷积操作是计算密集型操作，因此在设计张量卷积操作的微内核时，需要结合 CPU 指令级并行的特点排列微内核中指令操作的布局，以使微内核在执行时可以同时并行处理取出的多条指令操作，从而最大限度地提高 CPU 资源的利用率。

##### 线程级并行加速

线程级并行加速（Thread-Level Parallelism，TLP）的主要目的是通过同时执行多个线程来提高计算机系统的效率。

相对于指令级并行加速，线程级并行加速更注重并行处理多个任务或者应用程序的能力。

在一个多核处理器中，每个核心都有自己的线程调度器，负责管理和调度该核心上正在运行的线程。

该文中所使用的多线程编程环境是 OpenMP将张量卷积算法转换为一个适合并行执行的模型，并使用 OpenMP 的 Fork/Join 模式实施并行化（如图 3-3 所示）。

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921143910.png)

##### 数据级并行加速

数据级并行加速（Data-Level Parallelism，DLP）是一种利用多个处理单元同时执行同一个操作的并行计算技术。

在数据级并行加速中，大量的数据被切分成若干个小块，每个小块都可以被独立地操作。这些小块可以分配给不同的处理单元进行并行计算，从而有效地利用了多核处理器的能力。相对于指令级并行加速和线程级并行加速，数据级并行加速更注重并行化处理数据的能力。

数据级并行加速常用于需要处理大规模、密集型的数据集的应用程序中，如图像处理、信号处理和机器学习等领域。在这些领域中，大量的数据需要进行复杂的运算和变换，并且这些运算和变换通常都可以被分割成许多小的、独立的操作。通过将这些小操作分配给不同的处理单元并行执行，可以显著提高程序的效率，缩短运行时间。

在本文设计实现的张量卷积算法中，在张量数据的批次维度对数据进行了初步划分，之后将不同列的连续点乘窗口作为基本单位划分数据块，在确保不同线程的操作之间没有数据依赖的前提下尽可能得提高单个线程的连续操作的数据重用

### CPU 上张量卷积算法的实现及优化

#### 基于 im2win 变换的张量卷积实现

据算法 3 所定义的基于 im2win 变换的张量卷积，该算法可被 CPU 上的串行结构有效映射，因而使得实现此算法的 CPU 串行逻辑与算法 3 的基本结构高度一致。

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921151411.png)

#### 基于 im2win 变换的张量卷积算法优化设计

上节提出的基础的基于 im2win 变换的张量卷积算法存在许多局限性和瓶颈，计算效率和访存效率都不高。从分析模型、循环重排列、循环展开、向量化、FMA 指令、存储器分块和并行策略等多个方面来提出有效的优化设计方案，最终实现高性能的基于 im2win 变换的张量卷积算法，并使其适应于不同的卷积场景。

##### 张量卷积算法的分析模型

张量卷积是一种计算密集型操作，它与矩阵乘法有类似之处，因此可以参照矩阵乘法的分析模型建立张量卷积的分析模型。

该分析模型架构具有以下特点：

——向量寄存器。

——FMA 指令。

——加载/存储架构。

——带宽。

##### 基于循环重排列和循环展开优化的张量卷积

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921151655.png)

##### 基于向量化和 FMA 指令优化的张量卷积

##### 基于存储器分块优化的张量卷积

寄存器分块：为了实现算法的最大性能，需要尽可能多得将张量元素保存在寄存器中，并最小化 SIMD 向量加载次数。

高速缓存分块：在具有更多级别的内存层次结构中，例如具有高速缓存的架构中，因此可以进一步将 im2win 张量 划分为更小的分区，以使其适合高速缓存的分块级别。

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921152103.png)

##### 基于并行策略优化的张量卷积

在并行计算中，“易并行计算（an embarrassingly parallel）”被定义为可以轻松将整个任务分成许多并行任务的工作量。

## 基于 im2win 变换的张量卷积在 GPU 平台上的优化实现

###  GPU 平台的体系结构

图形处理器（GPU）是一种高并发、高吞吐量和高效能的处理器，已经成为人工智能和深度学习领域的主要计算平台。

####  GPU 的硬件结构

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921154107.png)

由于高度并行的架构设计，现代 GPU 在图像和图形处理方面展现了极高的效率。相比之下，通用处理器 CPU 更适合处理逻辑复杂的任务。

从硬件结构角度来看，CPU 和 GPU 存在很大差异。CPU 拥有众多功能模块，可以适应复杂的运算环境。而 GPU 构成相对简单，其中流处理器和显存控制器占据了绝大部分晶体管。

从指令级别来看，CPU 和 GPU 也存在巨大差异。CPU 由专门为顺序串行处理而优化的少量核心组成。而 GPU 则由数以千计的更小、更高效的核心组成，并且这些核心专门为同时处理多任务而设计。

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921154347.png)

GPU 之所以适用于处理数据并行计算任务，是因为它拥有大量简化了复杂逻辑控制单元和缓存体系的计算核心。这使得 GPU 可以同时执行数千个线程，并在大规模数据并行问题上表现显著优于CPU。

#### GPU 的软件编程模型

GPU 的软件编程模型主要有 OpenCL、CUDA等。该论文采用NVIDIA的GPU解决方案，并基于其 CUDA 架构实现张量卷积及优化。

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921154541.png)

##### CUDA 架构

统一计算设备架构（Compute Unified Device Architecture，CUDA）是NVIDIA 推出的一种利用 GPU 进行通用并行计算的平台和编程模型。

包含了专为 GPU 计算设计的指令集架构和内部并行计算引擎。

该架构具有以下三个关键抽象特性：1、线程组层次：将大量线程组织成网格，每个网格由多个线程块组成，每个线程块又包含多个线程。这种划分使开发人员可以根据数据并行度和硬件资源来安排线程执行。2、共享内存：位于每个多处理器上、可被同一个线程块中所有线程共享的高速缓存，由开发人员显式管理。共享内存可以提高数 据 访 问 速 度 ， 并 减 少 对 设 备 内 存 的 传 输 。 3 、 屏 障 同 步 机 制 ： 通 过__syncthreads()函数实现，在该函数处，所有线程都会等待直到同一个线程块中的所有其他活动线程都到达该点。

##### CUDA 线程层次

在 CUDA 架构中，为了充分利用 GPU 的并行性能，线程层次被设计为网格、线程块和线程三个不同的级别。

网格（grid）表示一个核函数的启动，线程块（block）表示在一个流多处理器（Streamimg Multiprocessor, SM）中执行的一组线程，而线程（thread）则表示最小的计算单元。每个线程都执行相同的代码，并且可以访问同一块共享内存。线程块中的所有线程可以通过屏障同步机制进行协调。

在硬件层面上，每个 SM 以每 32 个线程为一组创建、管理、调度和执行这些线程，并称之为 warp。当一个 SM 被分配有一个或多个线程块时，则会被分成多个 warps，并且每个 warp 都由调度器进行调度。

##### CUDA 内存模型

CUDA 内存模型是一种显式的内存模型，可以通过它控制 GPU 中不同层次的内存空间。CUDA 内存模型包括以下几种类型的内存：

——全局内存：GPU 中最大的一块存储器，用于存储所有线程都可以访问的数据。访问速度较慢，应尽量减少对它的访问。

——共享内存：每个 SM 都有一定数量由线程块分配的共享内存，用于线程块内部的线程协作。共享内存可以提高程序性能，避免访问全局内存，但是需要同步机制来防止数据冲突。

——L1 和纹理缓存：用于缓存最近访问过的数据，加快 GPU 程序执行速度。L1 缓存在每个 SM 上，纹理缓存在每个流多处理器上。

——L2 缓存：用于处理较大规模计算问题时使用。L2 缓存在所有 SM 之间共享，比 L1 缓存在更大但是相对较慢。

——寄存器：GPU 中最快的一种类型的内存，每个线程都有自己独立的寄存器文件。寄存器通常用于保存频繁访问和计算结果等变量。

#### GPU 的优化技术

##### 共享内存和寄存器

共享内存是 GPU 中每个线程块内部的高速缓存，可被该线程块中的所有线程共享访问。它可以降低对全局内存的依赖，提高数据局部性和并行效率。寄存器是 GPU 上最快速的内存类型，为每个线程提供临时变量空间。由于寄存器数量有限，需要合理分配和使用寄存器，避免发生寄存器溢出。

##### 数据并行性

GPU 是一种基于数据并行模型设计的处理器，能够同时执行多个相同或不同的任务。为了充分利用 GPU 的并行能力，需要保证程序中有足够多的独立可并行执行的数据。这就要求在编写程序时避免不必要的串行操作和同步操作，尽量使用向量化指令和循环展开等技术来增加指令级并行度。

**提高占用率**

占用率是指一个线程块中活跃线程数占最大线程数（通常为 32 或 64）的比例。占用率越高，表示 GPU 上空闲资源越少，吞吐量越大。提高占用率可以通过合理设置线程块大小、寄存器数量、共享内存大小等参数来实现。

##### 合并访存和内存对齐

合并访存和内存对齐合并访存是指多个线程同时访问连续或相邻地址时，将这些访问合并成一次或几次较大粒度的访问。这样可以减少总体访问次数和延迟，并提高带宽利用率。为了实现合并访存，需要保证数据在全局内存中是连续或相邻排列，并且与缓冲区边界对齐。

##### 快速指令和指令级并行

快速指令是指执行时间较短或没有延迟开销的指令。例如，在 CUDA 中有一些特殊函数叫做 intrinsics，它们可以直接映射到硬件操作而不需要调用库函数。使用快速指令可以减少运算时间和节省资源。另外，在 GPU 上执行时，每个线程块会被划分为若干个 warp（通常为 32 个线程），每个 warp 会以单指令多数据（SIMD）方式执行相同或不同的指令流。为了提高 warp 级别的并行度，需要避免分支散度和控制流散度等情况发生。

### GPU 上张量卷积算法实现

GPU 平台的可用内存远远小于 CPU 平台的可用内存，因此，在 GPU 平台上有效降低卷积算法的内存占用更加关键。

#### 张量卷积算法的索引映射

GPU 平台的 CUDA 编程模型架构能够有效地并行化循环结构，但其维度数量有限，每个维度的取值范围较大。在算法 3 中，共存在 7 个 for 循环的维度，其中各个维度的取值范围通常较小。这意味着算法 3 中的卷积结构难以直接映射到 CUDA 架构上（因为 CUDA 架构的维度数量较少，但每个维度的取值范围通常较大，与算法 3 中情况相反）。

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921162720.png)

原 本 采取的 7 层 嵌 套 循 环 结 构 被 重 新 映 射 到 了 3 个 维 度 上。

####  CUDA 架构上的张量卷积算法

在整个张量卷积算法中，不同输出张量*O*之间是相互独立的，因此可以将输出张量的维度映射到 CUDA 模型的线程上。由于输出张量*O*的维度只与*M*和*N*两个循环有关（算法 7 中第 1行至第 6 行），因此可以将这两个循环映射到 CUDA 模型中的不同线程上。

### GPU 上张量卷积算法优化设计

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921164036.png)

在算法 8 中，由于所有的数据都存储在全局内存中，导致在全局内存上的读取以及操作具有很高的延迟，从而导致该算法性能表现不佳。为了提高算法性能，本节将介绍针对张量卷积算法的优化设计。这些优化技术包括 tiling、共享内存和寄存器、微内核设计、向量化加载/存储、双缓冲和预取等技术。可以最大化工作负载和数据并行性，并减少数据访问延迟。

![](E:\研究生时用\论文笔记\笔记图片\QQ截图20230921165221.png)

#### 基于 tiling 策略优化的张量卷积

Tiling 技术是一种基于数据并行性的优化技术，旨在通过将数据划分为更小的块（称为 tile）来减少内存访问冲突和提高内存带宽利用率。

在 GPU 上，tile 块通常被划分为线程块级别和线程级别，作为基本的计算单元来执行计算任务。

#### 基于共享内存和寄存器优化的张量卷积

算法 9 中，将 im2win 张量和卷积核张量划分为 tile 块之后，可以为每个线程分配大小为MtXNt的寄存器，而为每个线程块分配大小为MbXNb和KbXNb的共享内存（算法 9 中第 1 行至第 2行），并将位于全局内存中的 im2win 张量和卷积核张量加载到共享内存和寄存器中（算法 9 中第 3 行至第 7 行）。由于共享内存由线程块内的所有线程共享，因此将数据在共享内存中重用远快于在全局内存中重用，这可以显著提高性能。

#### 基于微内核设计优化的张量卷积

微内核技术是一种基于微架构设计的优化技术，通过将 GPU 的硬件架构分解为多个小型、独立的内核来实现，从而提高 GPU 的并行性和性能。

#### 基于向量化加载/存储优化的张量卷积

向量化加载/存储是一种在 GPU 上提高内存访问效率的技术，其主要目的是通过将多个数据元素一次性地加载或存储到寄存器中，从而改善数据 I/O 效率和内存带宽利用率。

在 GPU 上进行卷积计算时，数据 I/O 和内存带宽通常是性能瓶颈之一。针对这一问题，上文提出了 im2win 变换，即将输入张量转换为连续的窗口排序的 im2win 张量，以便于使用向量化加载将输入张量中的数据一次性地加载到寄存器中（算法 9 中第 6 行至第 7 行），并在寄存器中进行后续计算。

####  基于双缓冲和数据预取优化的张量卷积

双缓冲技术是指使用两个缓冲区来存储输入张量和滤波器张量以进行流水线并发计算。

在算法 9 中，为共享内存和寄存器时分配了两倍的空间（算法 9 中第 1 行至第 2 行），其中一个缓冲区用于计算，另一个用于加载下一次计算中使用的新数据。当计算完成后，两个缓冲区的角色会交换，即原始缓冲区变为新的加载缓冲区，原始加载缓冲区变为新的计算缓冲区。

数据预取技术是通过将下一次卷积计算使用的张量数据提前加载到寄存器（或共享内存）中，以隐藏从全局内存读取数据的延迟和开销（算法 9 中第 10 行至第 11 行和第 14 行至第 15 行），从而减少等待时间并提高计算效率。

